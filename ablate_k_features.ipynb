{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/log/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load dictionary\n",
    "# Need model definition, right?\n",
    "import torch.nn as nn\n",
    "from baukit import Trace\n",
    "from einops import rearrange\n",
    "\n",
    "# import partial\n",
    "from functools import partial\n",
    "# from src.autoencoder import UntiedSAE\n",
    "import torch\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# dictionary_path = \"models/pythia-70m-deduped_gpt_neox.layers.3_Ratio-4_l1-0.003_lr-0.001_2024_02_27_17_47_39_model.pt\"\n",
    "dictionary_path = \"models/pythia-70m-deduped_gpt_neox.layers.3_Ratio-4_l1-0.003_lr-0.001_2024_02_27_19_24_10_model.pt\"\n",
    "layer = 3\n",
    "activation_name = f\"gpt_neox.layers.{layer}\"\n",
    "sae = torch.load(dictionary_path).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "UntiedSAE()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "from src.utils import get_dataloader\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "model_name = \"EleutherAI/pythia-70m-deduped\"\n",
    "dataset_name = \"Elriggs/openwebtext-100k\"\n",
    "# dataset_name = \"jbrinkma/pile-500k\"\n",
    "batch_size = 16\n",
    "context_length = 256\n",
    "# model and dataset\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "train_loader, test_loader = get_dataloader(dataset_name, tokenizer, batch_size, context_length)\n",
    "# Freeze the model & dictionary\n",
    "model.eval()\n",
    "model.requires_grad_(False)\n",
    "sae.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_k_lowest_nonzero(arr, k):\n",
    "    abs_arr = arr.abs()\n",
    "    # Set zeros to +inf to ignore them in topk (since we are now looking for lowest values)\n",
    "    mask = torch.where(abs_arr > 0, abs_arr, torch.tensor(float('inf'), device=arr.device))\n",
    "    # Get top k lowest values; since torch.topk only finds the largest, we use -mask\n",
    "    _, indices = torch.topk(-mask, k, dim=1)\n",
    "    # Create a range for the batch dimension\n",
    "    batch_indices = torch.arange(arr.size(0), device=arr.device).unsqueeze(1)\n",
    "    # Use advanced indexing to set the k-lowest nonzero elements to 0\n",
    "    arr[batch_indices, indices] = 0\n",
    "    return arr\n",
    "    \n",
    "def dict_ablation_fn_k(representation, name, other_value, k=0):\n",
    "    # print(f\"representation {representation}\")\n",
    "    if(isinstance(representation, tuple)):\n",
    "        second_value = representation[1]\n",
    "        internal_activation = representation[0]\n",
    "    else:\n",
    "        internal_activation = representation\n",
    "    int_val = rearrange(internal_activation, \"b seq d_model -> (b seq) d_model\")\n",
    "    features = sae.encode(int_val)\n",
    "    features = zero_k_lowest_nonzero(features, k)\n",
    "    reconstruction = sae.decode(features)\n",
    "        \n",
    "    batch, seq_len, hidden_size = internal_activation.shape\n",
    "    reconstruction = rearrange(reconstruction, '(b s) h -> b s h', b=batch, s=seq_len)\n",
    "\n",
    "    if(isinstance(representation, tuple)):\n",
    "        return_value = (reconstruction, second_value)\n",
    "    else:\n",
    "        return_value = reconstruction\n",
    "\n",
    "    return return_value\n",
    "       \n",
    "def compute_loss(inputs_ids, logits):\n",
    "    return torch.nn.CrossEntropyLoss()(\n",
    "        logits[:,:-1,:].reshape(-1, logits.shape[-1]),\n",
    "        inputs_ids[:,1:].reshape(-1)\n",
    "    ).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([8.0784, 8.0807, 8.0873, 8.1022, 8.8858])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load in N batches\n",
    "N = 2\n",
    "total_k = 5\n",
    "avg_CE = torch.zeros(total_k)\n",
    "\n",
    "d_model, n_features = sae.W_d.shape\n",
    "tokens_per_batch = batch_size * context_length\n",
    "all_features = torch.zeros(tokens_per_batch*N, n_features)\n",
    "with torch.no_grad():\n",
    "    for ind_batch, batch in enumerate(train_loader):\n",
    "        if ind_batch >= N:\n",
    "            break\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        for k_inc in range(total_k):\n",
    "            # k_inc = 0\n",
    "            hook_function = partial(dict_ablation_fn_k, k=k_inc)\n",
    "            with Trace(model, activation_name, edit_output = hook_function) as ret:\n",
    "                #Calculate CE\n",
    "                logits = model(input_ids).logits\n",
    "                CE = compute_loss(input_ids, logits)\n",
    "                avg_CE[k_inc] += CE\n",
    "\n",
    "                # #Calculate MSE\n",
    "                # representation = ret.output\n",
    "                # if(isinstance(representation, tuple)):\n",
    "                #     representation = representation[0]\n",
    "                # activation = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "                # reconstruction = sae(activation)\n",
    "                # # features = sae.encode(activation)\n",
    "                # # all_features[ind_batch*tokens_per_batch:(ind_batch+1)*tokens_per_batch] = features\n",
    "                # MSE = torch.nn.MSELoss()(reconstruction, activation).item()\n",
    "                # # print k, mse, ce\n",
    "                # print(f\"K, MSE, CE: {k_inc}, {MSE}, {CE}\")\n",
    "                # avg_MSE[k_inc] += MSE\n",
    "        # outputs = model(input_ids)\n",
    "avg_CE /= N\n",
    "print(avg_CE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K, MSE: 0, 0.15783685445785522\n",
      "K, MSE: 1, 0.15818116068840027\n",
      "K, MSE: 2, 0.15910932421684265\n",
      "K, MSE: 3, 0.16127437353134155\n",
      "K, MSE: 4, 0.16592943668365479\n",
      "K, MSE: 0, 0.15939773619174957\n",
      "K, MSE: 1, 0.1597251147031784\n",
      "K, MSE: 2, 0.16059614717960358\n",
      "K, MSE: 3, 0.16243727505207062\n",
      "K, MSE: 4, 0.16649633646011353\n",
      "tensor([0.1586, 0.1590, 0.1599, 0.1619, 0.1662])\n"
     ]
    }
   ],
   "source": [
    "# Load in N batches\n",
    "N = 2\n",
    "total_k = 5\n",
    "avg_MSE = torch.zeros(total_k)\n",
    "\n",
    "d_model, n_features = sae.W_d.shape\n",
    "tokens_per_batch = batch_size * context_length\n",
    "all_features = torch.zeros(tokens_per_batch*N, n_features)\n",
    "with torch.no_grad():\n",
    "    for ind_batch, batch in enumerate(train_loader):\n",
    "        if ind_batch >= N:\n",
    "            break\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        for k_inc in range(total_k):\n",
    "            # k_inc = 0\n",
    "            with Trace(model, activation_name) as ret:\n",
    "                #Calculate CE\n",
    "                _ = model(input_ids).logits\n",
    "                #Calculate MSE\n",
    "                representation = ret.output\n",
    "                if(isinstance(representation, tuple)):\n",
    "                    representation = representation[0]\n",
    "                activation = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "                features = sae.encode(activation)\n",
    "                features = zero_k_lowest_nonzero(features, k_inc)\n",
    "                reconstruction = sae.decode(features)\n",
    "\n",
    "                # features = sae.encode(activation)\n",
    "                # all_features[ind_batch*tokens_per_batch:(ind_batch+1)*tokens_per_batch] = features\n",
    "                MSE = torch.nn.MSELoss()(reconstruction, activation).item()\n",
    "                # print k, mse, ce\n",
    "                print(f\"K, MSE: {k_inc}, {MSE}\")\n",
    "                avg_MSE[k_inc] += MSE\n",
    "        # outputs = model(input_ids)\n",
    "avg_MSE /= N\n",
    "print(avg_MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  273, 13175,   432,   247,  1077,  2969,  5673,    15, 48627,    13,\n",
      "           352, 10262,   368,   342,   253,  3745,   281, 10007,   253,  1655,\n",
      "         10611,   275,   247,  1180,   273,  4722,  4088,    13,  1690,  4933,\n",
      "          2289,   281,   247,  1180,   273,  2969,  5657,   323, 42477,   941,\n",
      "         21453,    15, 32354,  6240,   281,   253,  1618,   273,  5667,    84,\n",
      "            15,   187,   187,  2598,   627,   597,   403,    15,  1310,   368,\n",
      "           452,   667,   309,   943,   823,   281,   436,  1618,   273,  5667,\n",
      "            84,    13,   513,  1339,   479,   871,   275,   253,  5701,   390,\n",
      "          3066,  4579,    15, 12590,   187,   187,     9,  8061,   281,  1110,\n",
      "           665,  6518,   479,  1973,   436,  1618,  1690, 41474, 18039,  5969,\n",
      "           285, 11819,   330,  1761,    80,    10,   187,   187, 20536,   187,\n",
      "           187,    60,  8339,  9044, 36304,  1040, 16447,   928,  7373,   405,\n",
      "          2146,    15,   681,  5032,     0, 18412,   253,  4422,   247,  8511,\n",
      "           991,   512,  2112,    32,   187,   187,    39,   507,   273,   399,\n",
      "          8635,   367,  3938,   403, 37986,   347,  3063, 10880,   672,   247,\n",
      "          4422, 14163,   873,   281, 10578,   616,  4240,  1533,  4892,   369,\n",
      "           873,   281,  8027,    15,   187,   187, 31840,   436,  1770,    13,\n",
      "           247,   747,  4422,  5420,  3909,   534,  1142,  6566,   369,   873,\n",
      "           598,   407,   253, 15688, 32458,   281, 10578,   616,  1091,  1735,\n",
      "           807,    15,   380,  4422,  2802,  2422,   422,  4240,   457,  3977,\n",
      "          7458,   281,  2868,   326,   247,  1533,  4892,   432,   253,  2802,\n",
      "          3633, 45023,   457,  6114,   369, 31771,    15,   399,  8635,   367,\n",
      "          3938,   452,   767,  3153, 16258,  1925,  2802,  2422,   422,  8210,\n",
      "           457,   285,  2802,  2422,   422,  5215,   457,  4907,   846,   616,\n",
      "           767, 24478,    13,   534,  1646,   281,  5108,  2378,   247,  9976,\n",
      "          1108,  6240,   281,   253,  3762,   326],\n",
      "        [  320,  2403,  2119,   326,  3253,   310,   275,  1659,     3,   281,\n",
      "          1361,  1110,  5876,   407,   253,  9902,    13,  6729,  2183,  4544,\n",
      "          5820,   275, 27687,    13, 28675,   904,   327,  6926,  2360,    15,\n",
      "           754,   753,   253,  9902,  5486,   344,   651,   346,  1439,   320,\n",
      "          2104,   281,  4544,  3240,   347,  1199,   689,   253,  1735,  1643,\n",
      "          1897,   449,   187,   187, 16812,    13,  4172,  2784,   598,  6729,\n",
      "           434,  3483,  1037,  9355,  4288, 10130,   369,  1633,   521,  8068,\n",
      "            14,  3169,  4544,   369,  2343,   506,   281,   513,  5734,  8839,\n",
      "          3309,    15,   187,   187,   688,   253,  6863,  5492,    13,   253,\n",
      "          9183,   452,  1643,  9091,  1669,   281,   787,  5432,   949,   253,\n",
      "           954, 12085,  3054,    13,  2820,   281,  1973, 10254,   285,  1056,\n",
      "           247,  2457, 11288,   281,   440, 31572, 11163,    15,   187,   187,\n",
      "           510,  4007,   434, 10885,   273,   253,  9902,   812, 26740,  1110,\n",
      "          3563,    14, 22071, 11163,    15,  1310,  6729,   310, 12351,   347,\n",
      "           247,  2266,  6657,   665,  2722,  3923,   275,   247,  8891,    13,\n",
      "           690,   440, 31572, 11163,   778,   320, 25042,   281,   896,   253,\n",
      "          4007,    15,  1292,   247, 17994,  2147,  2380,   390,   247,  3282,\n",
      "           326,   344,   434,  8133,  8672,   689,  1345,  5252,   812, 20171,\n",
      "           521,  1329,   387,   247,  1127,   275,   253,  5492,   835,   627,\n",
      "           434,  1652,  4839,   281,  8107,  2282,    15,   187,   187,     3,\n",
      "            42,  1158,   326,   253,  4007,   273,   253,  1986,  2077,   310,\n",
      "           253, 17747,   275,  7015,    15,   380,  2448,   952,  1007,   281,\n",
      "           779,    13,   285,   309,  1353,  2119,   344,   588,  2589,  2994,\n",
      "           285,  1132,   521,  9550,  2554,   275,   247,  4030,  8142,    15,\n",
      "          1893,   309,   651,  8564,   326,  1537,  1361,   779,   247,  1652,\n",
      "          2372,   937,   753, 13169,  4673,    15],\n",
      "        [  247,  2454,    14, 40133,  2557,   846,  3995, 26587,  2516, 50061,\n",
      "          1216,   327,  8216,  7320,   253,  3236,  6007,   987,  1078,   352,\n",
      "           369,  6326,   281,  4763,   247,  6273,    15,  1292,   436,  6326,\n",
      "         18485, 10170,   310,  6571,   247, 34209, 21850,    15,   380,  8907,\n",
      "           556,  2168,  1669,  3874,   323,   253,  4223, 24096,    13,   285,\n",
      "           253,  5170,  9320,  2779,  5082,   457,    85,   452,  7012,   253,\n",
      "          3995,   457,    84,  5680,  6007,  8791,    13,   342, 10823, 15081,\n",
      "           352,  2506,   457,    85,  3959,  2217,  3081,  8064,   285,  4428,\n",
      "         17770, 10067,  1842,   272,   253, 36759,   273,  2151,    15,   187,\n",
      "           187,    35,  3703,    73,  1216,  7303,   281,  3330,   896,   253,\n",
      "         10331,  3128, 12916,   273,   521,  3128,   407, 25015,   253,  9100,\n",
      "          6007,   342,   247, 34614,   350,  2557, 14572,   896,  3918, 22306,\n",
      "          6729,   457,    84,  3366, 13004, 12121,   323,  8347,  3639,  1780,\n",
      "          1069,   932,   313,    37, 33550,    10,  2086,    13,   271,  9165,\n",
      "          1340,   326,  7169,   767,    14,  2913,  5172,  1564,   281, 42178,\n",
      "            13,   933,   594,    14,  8890,   399, 30704,   398,    13,  2872,\n",
      "         46098, 16618,   665,  8899,   598,   275,   253,  1982,    15,   380,\n",
      "          3270,    14,    37, 33550,  6007,  1128,  4609,  4817,   407,   247,\n",
      "         24521,    14, 14403,  6273,  1996,  6794,  2360,  1128, 12756,  2534,\n",
      "           253,  6729,  5286,   432,  9159,   747,   399, 33550,  5172,  1564,\n",
      "           390, 10660,   272,  1655,   399, 33550, 35029,   672,   616,   767,\n",
      "          1107,   403,   598,    15,   831,  6007,   369,   247,   921,  6273,\n",
      "            13,   323,   253,  8907,   285,  6729,   651,  1620,   564,  2112,\n",
      "           342,   352,    15,  1292,   436,   369,   253, 18485,  3935,   273,\n",
      "           253,  1388,    27,   275,  1340,   281,  3330,   253,  1329,   273,\n",
      "         10331,  3128, 11885,   323,   247,  6007]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    print(input_ids[:3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'input_ids': tensor([ 1335,  2589,  3187, 21269,   398,   281,  3187,  3943,  5871,    13,\n",
       "            533,  2550,  2430,   247,  5927, 35958,  2408,   591, 42307,   390,\n",
       "           2021,   285,  2550, 29211, 16947,   281,  2060,  2758,    15,   733,\n",
       "            457,    84,  1335, 10826,   281,  2186,  3187, 21269,   398,   285,\n",
       "            299,  1513,   782,  8064,   323,  4712,   326,  5649,   253,  3114,\n",
       "             13,   824,   347, 26864,  2579,  6493,    13,   390,   281,  7164,\n",
       "           2583,   323,   247,  2173,  3943,  4096,   824,   347,   253,  7471,\n",
       "            273,   247, 19150,   390,  6500,    15,  1244,  5085,   476,  1335,\n",
       "           6558,   773, 17703,  8553,   668,   323,  1810,  8349,   285,  5870,\n",
       "            604,   597,  5730,   281,  1978,  2583,   327, 18831,   323,  7830,\n",
       "            273, 12222,    15,   187,   187,  1552,   778,   320,   247,  2201,\n",
       "           8230,   281, 10824,   285,  1014, 30171,  2439,   253,  2586,   347,\n",
       "            597,  2968,   342,   253,  3486,   327,   616, 14199,   285, 35905,\n",
       "             15,  6676,  3533,  3464,   440, 42195,    27,   187,   187,  9586,\n",
       "            604,   247,  8326,  2122,  6003,  3548,   457,    85,   247, 28416,\n",
       "              9,    68,  2769,    20,    10, 34251,  6003,   313,  9088,   403,\n",
       "           2067,    13,   824,   347, 13240,   390,  3289, 20036,   582, 35098,\n",
       "            457,    84,  4618,    14, 45017, 23069, 30034,   651,  1646,   281,\n",
       "           8913,  5085,   326,  5082,   457,    85,  5010,   320,  5876,   432,\n",
       "           2060, 35958,    15,   187,   187, 20696,   253, 25275,   281,  7164,\n",
       "           8064,   323,   616,  8553,    13,   849,   513,   359, 41509,  1810,\n",
       "           8349,   285,   616,  5870,   281, 10078,   275,  3187, 21269,   398,\n",
       "           1108,  3340,   776,   625, 24683,  5014,    32,   187,   187,  5804,\n",
       "           1810,  8349,  1335,  2997, 35186,   323,   616, 11497,    13,   824,\n",
       "            347,  1110,  5431,  1677,   323,  2710,  2308,   273,  1684, 30736,\n",
       "           6224,    32,   187,   187,  2214,  1142])},\n",
       " {'input_ids': tensor([ 1335,  2589,  3187, 21269,   398,   281,  3187,  3943,  5871,    13,\n",
       "            533,  2550,  2430,   247,  5927, 35958,  2408,   591, 42307,   390,\n",
       "           2021,   285,  2550, 29211, 16947,   281,  2060,  2758,    15,   733,\n",
       "            457,    84,  1335, 10826,   281,  2186,  3187, 21269,   398,   285,\n",
       "            299,  1513,   782,  8064,   323,  4712,   326,  5649,   253,  3114,\n",
       "             13,   824,   347, 26864,  2579,  6493,    13,   390,   281,  7164,\n",
       "           2583,   323,   247,  2173,  3943,  4096,   824,   347,   253,  7471,\n",
       "            273,   247, 19150,   390,  6500,    15,  1244,  5085,   476,  1335,\n",
       "           6558,   773, 17703,  8553,   668,   323,  1810,  8349,   285,  5870,\n",
       "            604,   597,  5730,   281,  1978,  2583,   327, 18831,   323,  7830,\n",
       "            273, 12222,    15,   187,   187,  1552,   778,   320,   247,  2201,\n",
       "           8230,   281, 10824,   285,  1014, 30171,  2439,   253,  2586,   347,\n",
       "            597,  2968,   342,   253,  3486,   327,   616, 14199,   285, 35905,\n",
       "             15,  6676,  3533,  3464,   440, 42195,    27,   187,   187,  9586,\n",
       "            604,   247,  8326,  2122,  6003,  3548,   457,    85,   247, 28416,\n",
       "              9,    68,  2769,    20,    10, 34251,  6003,   313,  9088,   403,\n",
       "           2067,    13,   824,   347, 13240,   390,  3289, 20036,   582, 35098,\n",
       "            457,    84,  4618,    14, 45017, 23069, 30034,   651,  1646,   281,\n",
       "           8913,  5085,   326,  5082,   457,    85,  5010,   320,  5876,   432,\n",
       "           2060, 35958,    15,   187,   187, 20696,   253, 25275,   281,  7164,\n",
       "           8064,   323,   616,  8553,    13,   849,   513,   359, 41509,  1810,\n",
       "           8349,   285,   616,  5870,   281, 10078,   275,  3187, 21269,   398,\n",
       "           1108,  3340,   776,   625, 24683,  5014,    32,   187,   187,  5804,\n",
       "           1810,  8349,  1335,  2997, 35186,   323,   616, 11497,    13,   824,\n",
       "            347,  1110,  5431,  1677,   323,  2710,  2308,   273,  1684, 30736,\n",
       "           6224,    32,   187,   187,  2214,  1142])})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader.dataset[0], train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  285,   247,  2495,  2803,   323,  8401,    15,  8969, 23787,  2296,\n",
      "          2802,   423,  1417,  3038,  5262, 32412,  3390,    13, 17120,   390,\n",
      "          1469,   281,   253, 17409,   812,   320,   247,  8138,   273,   673,\n",
      "           323,   598,   281,   581,    14, 25512,   394,   273,   253,  3072,\n",
      "         15385,   187,   187, 20576,    13,   597,   943,   513,  1029,  7133,\n",
      "            13, 12217,  7467,  5763,   824,   347, 14174,   390,  2801,  3733,\n",
      "            15,   187,   187,  1717,    15,  1889,  2269,  2182,  3304,    15,\n",
      "          5595,  2175,   921,   326,    13,  2429,   342, 30347, 42842,    13,\n",
      "         30347,   275,  3626, 12620,   310,  2330,   342,  3687,  9510,    84,\n",
      "           281, 12315,    13,  6137, 10397,   285,   271,  2559, 12177,   273,\n",
      "         26530,   342,   253,   789,  8349,    15,   187,   187,  1229, 38733,\n",
      "           253,  3430,  5763,   323,   634,  2363,    15,  4325,  1475,   776,\n",
      "          1884,    84,    13,   359,  7168,   327,  3388,   581,    14, 25512,\n",
      "           394,   273,   247, 21059,   273,  6616,   247,   807,    13,  6701,\n",
      "           281,   247,  1232,  1929,   347, 23649, 32889,    15,   187,   187,\n",
      "          8924,  2632,  3119,  3733,   275,  1798,  1905,   824,   347, 20284,\n",
      "         13461,   390,   465, 35189,    67,  7042,  1905,  4916,   347,    13,\n",
      "           604,   417,   625,    13,  1774,   685, 34796,   789,  8349,   347,\n",
      "           368,   755,  5662,   281,  8409,   841, 11655, 15933,  2296, 35028,\n",
      "         49536,    13,  1481,   273,   253, 48476, 12169,   323, 15000, 21856,\n",
      "           387, 39605, 28634,  2499,    15,   187,   187,  4967,   513,   368,\n",
      "           878,   253,  6616,    32,   733,   457,    84, 12232,   323, 13195,\n",
      "           745,  4688,    15,   187,   187,  1992,   755,  4944,    13,   368,\n",
      "           878,   281,  5878,  5763,   824,   347, 21834,   285,  3515,    13,\n",
      "           342,  2801,  3733,   390,  5052,   789,   187,   187,  2405, 38733,\n",
      "           760, 34796,   390,  2801,  3733,    15],\n",
      "        [  617,   327,   253,  1111,   390,   275,   253,  8576,    13,  6153,\n",
      "           747,  5772,    14, 19771,  2221, 19480, 20247,    15,   187,   187,\n",
      "          1552,  3929,   310,  5907,   762, 28283, 22695,  7981,    15,   733,\n",
      "           457,    84,  8261,   281,  1234,  2292,   763,   352,  9825,   347,\n",
      "          1048,   347,   863,  2382,  9015,   310,  2908,   285,   512,  4859,\n",
      "          3464, 15282,    15,   187,   187, 43963,    27,   831,  3929,   310,\n",
      "           417,  6034,   281,  2085,  3739,  7535,    13,  6120,   390,  1971,\n",
      "            15, 44913,  4469,  1060,   513,   417,  7933,  4887,  1110,   273,\n",
      "           411,  1170,  7717,   390,   697,  4750,    15,     0,  7941,  3016,\n",
      "          2354,  7858,   313, 32680,    10,   428,   380,  5284,  6398,  6704,\n",
      "           271, 14464,  1959,    14, 37719, 47448,   342, 17111,   327,  6794,\n",
      "           285, 14315,   352,   812, 16209,   625, 17634,   327, 16496,  5734,\n",
      "           354,    14, 27397, 28550,   769,   281,  5448,  1066,   253,  8891,\n",
      "           275,   253,  9268,   273,   253,  2586,   407,  7216,    15,   187,\n",
      "           187, 17624,   314,   846, 10884,   281,   611, 28017,   432, 30167,\n",
      "           835,   344,  6704,   253, 47448,    13, 20510,   375,   864,  7381,\n",
      "          6138,   327,   521,  4422,   326, 17111,   574,  6508,   247, 22907,\n",
      "         11342,   407,  2208,  5621,  1411,   354,    14, 27397,  2533,   255,\n",
      "           382, 28550,   407,  8187,  3038,  1919,   884,   268,    15,    78,\n",
      "            15,   327,  7216,    15,   187,   187, 42399, 24647,  3918,  8939,\n",
      "           287, 20510,   375,   864,  7381,  2210,   281, 30167,   281,   861,\n",
      "           247,  2080,    14, 45017,  5454,   285,  3569, 15850,  4345,   342,\n",
      "           253,  8215,   326,   556,   644,   387,   253,  2798,   273,  2607,\n",
      "           273, 18649,  7217,   285,   598,   248, 36596,   275,   521,  2586,\n",
      "            13, 10263,   271,  8993,  4322,   273,   773, 45283,  9099,   668,\n",
      "           432,  7422,    15,   187,   187, 35848],\n",
      "        [   51,    15,   831,   310,   417,  7445,  1580, 13818,  2974,    15,\n",
      "         25360,  2544,  3240,  7208,   285,   359,  1053,   457,    85,   971,\n",
      "           281,  2600,  8140,  5731,   776,   416,  2795,  5931,   281,  2395,\n",
      "           323,  1110,  2544,    15,   733,   651,   320,  7445,   604,   359,\n",
      "           812, 13584, 10341,  3541,   432,   436,  1127,   327,   285,  1056,\n",
      "           776,   416,  2795,  5931,   625,  9630,    15,   844,   671,  1335,\n",
      "           878,   247,  1039,   281,  2686,  1379,  1453,   689,   253,  2685,\n",
      "           273, 10636,   273,   253,  2086,  1580,   512,   359,   452,  2218,\n",
      "           594,  2080,   310, 13584,   690,  3541,    15,   187,   187,  1394,\n",
      "          1537,  1158,   359,   476,   816,  1818,   253,  1318,   273,   776,\n",
      "          3731,  2132,  2876, 11104,  8479,   352,   281,   313,  2003,  1365,\n",
      "            10, 50029,   253,  2133, 11104,  2829,   352,  2792,   281,    13,\n",
      "           285,   436,   310,  2032,    13,   533,   417,   347,  4951,  3579,\n",
      "           347,   368,  1537,  1158,    15,  1281,    84,   816,  1611,   352,\n",
      "           285,   923,   752,  6569,    15,  2732,  4645,   253, 10028,  3817,\n",
      "          4508,   253, 31347,  1491,   359,   823,   247,  2014, 25109,  1386,\n",
      "           281,  1818,   253,  1318,   273,   253, 11104,    27,   187,   187,\n",
      "          6441,    60,    75,  1570,  1178, 11756,  1587,  9573,     3,   559,\n",
      "           891,    13,   346, 28597, 28597, 28597, 28597,  7246,    34,  2807,\n",
      "           187,   187, 31484,   436,   588,  6635,   253,  1563, 13035,   187,\n",
      "           187, 42993,  2228,  5189,   260,  1418, 24562,   313,  3046,    68,\n",
      "            15,   357,    21,  2262, 24037,  9775,  6517,   428,  2127,   854,\n",
      "          1418,  4838,   313,  7053,  4839,    10,   299,   991,    30,  4226,\n",
      "           299, 22141,    30,  4226, 10038,    89,    30,  2357,  9519,    67,\n",
      "            17,    66,  1407,    89,    30,  2941,  1036,    68,    21,    67,\n",
      "            22,  1578,    74,    30,  7174,    67]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for batch in train_loader:\n",
    "    input_ids = batch[\"input_ids\"].to(device)\n",
    "    print(input_ids[:3])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Index is supposed to be an empty tensor or a vector",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[67], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m nz \u001b[38;5;241m=\u001b[39m features \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# index by nz, keep as same shape\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnz\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Index is supposed to be an empty tensor or a vector"
     ]
    }
   ],
   "source": [
    "# Get the k-lowest nonzero features per datapoint\n",
    "# Feautures is shape (batch, features)\n",
    "features.shape\n",
    "nz = features != 0\n",
    "# index by nz, keep as same shape\n",
    "features.index_select(1, nz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 4., 5.],\n",
       "        [0., 2., 3., 0., 0.],\n",
       "        [1., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def zero_topk_nonzero(arr, k):\n",
    "    # Get absolute values to consider both positive and negative values\n",
    "    abs_arr = arr.abs()\n",
    "    # Use torch.where to create a mask of nonzero elements, setting zeros to -inf to ignore them in topk\n",
    "    mask = torch.where(abs_arr > 0, abs_arr, torch.tensor(float('-inf'), device=arr.device))\n",
    "    # Get top k values and their indices along the features dimension\n",
    "    _, indices = torch.topk(mask, k, dim=1, largest=False)\n",
    "    # Set the top k nonzero elements to 0\n",
    "    for i in range(arr.size(0)):  # Loop through batch dimension\n",
    "        arr[i, indices[i]] = 0\n",
    "    return arr\n",
    "\n",
    "# Example usage\n",
    "batch_size = 3\n",
    "num_features = 5\n",
    "k = 1\n",
    "arr = torch.tensor([[1, 2, 0, 4, 5], [0, 2, 3, 0, 0], [1, 0, 0, 1, 0]], dtype=torch.float32)\n",
    "result = zero_topk_nonzero(arr, k)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 2., 0., 4., 5.],\n",
       "        [0., 0., 3., 0., 0.],\n",
       "        [5., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def zero_k_lowest_nonzero(arr, k):\n",
    "    # Use torch.where to ignore zeros by setting them to +inf\n",
    "    mask = torch.where(abs_arr > 0, abs_arr, torch.tensor(float('inf'), device=arr.device))\n",
    "    # Get k smallest nonzero values and their indices along the features dimension\n",
    "    _, indices = torch.topk(mask, k, largest=False, dim=1)\n",
    "    # Set the k smallest nonzero elements to 0\n",
    "    for i in range(arr.size(0)):\n",
    "        arr[i, indices[i]] = 0\n",
    "    return arr\n",
    "\n",
    "# Example usage with the same array and parameters\n",
    "k = 1\n",
    "arr = torch.tensor([[1, 2, 0, 4, 5], [0, 2, 3, 0, 0], [5, 0, 0, 1, 0]], dtype=torch.float32)\n",
    "result = zero_k_lowest_nonzero(arr, k)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "        [0., 0., 0.,  ..., 0., 0., 0.]], device='cuda:0')"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def zero_k_lowest_nonzero(arr, k):\n",
    "    abs_arr = arr.abs()\n",
    "    # Set zeros to +inf to ignore them in topk (since we are now looking for lowest values)\n",
    "    mask = torch.where(abs_arr > 0, abs_arr, torch.tensor(float('inf'), device=arr.device))\n",
    "    # Get top k lowest values; since torch.topk only finds the largest, we use -mask\n",
    "    _, indices = torch.topk(-mask, k, dim=1)\n",
    "    # Create a range for the batch dimension\n",
    "    batch_indices = torch.arange(arr.size(0), device=arr.device).unsqueeze(1)\n",
    "    # Use advanced indexing to set the k-lowest nonzero elements to 0\n",
    "    arr[batch_indices, indices] = 0\n",
    "    return arr\n",
    "\n",
    "# # Example usage\n",
    "# batch_size = 3\n",
    "# num_features = 5\n",
    "k = 1\n",
    "# arr = torch.tensor([[1, 2, 0, 4, 5], [0, 2, 3, 0, 0], [5, 1, 0, 3, 0]], dtype=torch.float32)\n",
    "result = zero_k_lowest_nonzero(features.clone(), k)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.return_types.topk(\n",
       " values=tensor([4.1939, 2.7761, 2.1174, 2.0487, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       " indices=tensor([ 298,  721, 1571,   78,    7,    6,    4,    5,    3,    2,    0,    1],\n",
       "        device='cuda:0')),\n",
       " torch.return_types.topk(\n",
       " values=tensor([4.1939, 2.7761, 2.1174, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000,\n",
       "         0.0000, 0.0000, 0.0000], device='cuda:0'),\n",
       " indices=tensor([ 298,  721, 1571,    8,    0,    7,    5,    4,    6,    3,    1,    2],\n",
       "        device='cuda:0')))"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[1].topk(12), result[1].topk(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([4096, 2048]), torch.Size([81920, 2048]))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape, all_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "representation\n",
    "activation = rearrange(representation, \"b seq d_model -> (b seq) d_model\")\n",
    "# activation = rearrange(representation[:, 1:, :], \"b seq d_model -> (b seq) d_model\")\n",
    "\n",
    "# run through SAE\n",
    "features = sae.encode(activation)\n",
    "# get the k-lowest features per datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pos = 10\n",
    "scale = 1\n",
    "features_by_pos = [sae.encode(representation[:, (i*scale):(i*scale)+1, :].squeeze()) for i in range(num_pos)]\n",
    "# Plot each as a hist in a new plot\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "for i in range(num_pos):\n",
    "    # plot a new histogram\n",
    "    plt.figure()\n",
    "    zero_feature = features_by_pos[i].count_nonzero(-1)\n",
    "    plt.hist(zero_feature.cpu().numpy(), bins=20, alpha=0.5, label=f\"pos {i*scale}\")\n",
    "    # print(f\"topk features num {zero_feature.topk(10)}\")\n",
    "plt.title(\"L0 at different token positions\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 9,  9,  9,  7,  8, 10,  8,  8,  8,  9,  8,  9,  9, 10,  8, 10],\n",
       "        device='cuda:0'),\n",
       " torch.Size([16, 2048]))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zero_feature, features_by_pos[i].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the k-lowest features per datapoint\n",
    "k = 1\n",
    "# shape is (batch, features)\n",
    "d1 = features.count_nonzero(-1).cpu().numpy()\n",
    "d1.sort()\n",
    "# plot d1 as sorted line graph\n",
    "import matplotlib.pyplot as plt\n",
    "plt.hist(d1, bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([106, 107, 111, 114, 115, 116, 117, 117, 120, 122, 124, 127, 127,\n",
       "       130, 131, 132, 134, 134, 135, 136, 136, 137, 137, 137, 137, 138,\n",
       "       140, 141, 204, 234])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'sorted'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m d1\u001b[38;5;241m.\u001b[39msort(), \u001b[43md1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msorted\u001b[49m()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'sorted'"
     ]
    }
   ],
   "source": [
    "d1.sort(), d1.sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(8.)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.norm(all_features, 0, dim=-1).median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with Trace(model, activation_name, edit_output=dict_ablation_fn) as ret:\n",
    "    outputs = model(input_ids)\n",
    "    logits_dict_reconstruction = outputs[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
